{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Author**          | **Roll No**   | **Version** |\n",
    "|---------------------|----------------|--------------|\n",
    "| Abhyudaya Nair      | 24210005      | 1.0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "# Set up Chrome to run in headless mode\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode, which can help in some environments\n",
    "\n",
    "# Specify the location of the Chrome binary\n",
    "chrome_options.binary_location = \"/usr/bin/google-chrome\"  # Adjust this path if Chrome is installed elsewhere\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "base_url = \"https://www.bbc.com\"\n",
    "driver.get(base_url)\n",
    "\n",
    "# 1. Identify all the main sections from the homepage\n",
    "sections_links = []\n",
    "print(\"Identifying main topics (sections) from the homepage...\")\n",
    "\n",
    "# Main sections typically appear in the navigation bar or other major links\n",
    "main_sections = driver.find_elements(By.CSS_SELECTOR, 'a[href^=\"/news\"], a[href^=\"/sport\"], a[href^=\"/business\"], a[href^=\"/travel\"], a[href^=\"/culture\"]')\n",
    "for section in main_sections:\n",
    "    link = section.get_attribute(\"href\")\n",
    "    if link.startswith(base_url) and link not in sections_links:\n",
    "        sections_links.append(link)\n",
    "\n",
    "print(f\"Found {len(sections_links)} main sections to explore.\")\n",
    "\n",
    "# 2. Visit each section to identify subsections\n",
    "article_links = []\n",
    "\n",
    "for section_url in sections_links:\n",
    "    print(f\"\\nVisiting section: {section_url}\")\n",
    "    driver.get(section_url)\n",
    "    sleep(3)\n",
    "\n",
    "    # 3. Extract subsections from the section page\n",
    "    subsections_links = []\n",
    "    subsections = driver.find_elements(By.CSS_SELECTOR, 'a[href^=\"/news\"], a[href^=\"/sport\"], a[href^=\"/business\"], a[href^=\"/travel\"], a[href^=\"/culture\"]')\n",
    "    \n",
    "    for subsection in subsections:\n",
    "        subsection_link = subsection.get_attribute(\"href\")\n",
    "        if subsection_link.startswith(base_url) and subsection_link not in subsections_links:\n",
    "            subsections_links.append(subsection_link)\n",
    "\n",
    "    print(f\"Found {len(subsections_links)} subsections under {section_url}.\")\n",
    "\n",
    "    # 4. Visit each subsection\n",
    "    for subsection_url in subsections_links:\n",
    "        print(f\"\\nVisiting subsection: {subsection_url}\")\n",
    "        driver.get(subsection_url)\n",
    "        sleep(3)\n",
    "\n",
    "        # 5. Pagination: Go through each page in the subsection\n",
    "        while True:\n",
    "            print(f\"Collecting article links from page: {driver.current_url}\")\n",
    "            # Extract article links from the page\n",
    "            articles = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/news/\"], a[href*=\"/sport/\"], a[href*=\"/business/\"], a[href*=\"/travel/\"], a[href*=\"/culture/\"]')  # Adjust if other article patterns exist\n",
    "            for article in articles:\n",
    "                article_link = article.get_attribute(\"href\")\n",
    "                if article_link not in article_links:\n",
    "                    article_links.append(article_link)\n",
    "                    print(f\"Extracted article link: {article_link}\")\n",
    "            \n",
    "            # Check if there is a \"next\" button to go to the next page\n",
    "            try:\n",
    "                next_button = driver.find_elements(By.CSS_SELECTOR, '[data-testid=\"pagination-next-button\"]')\n",
    "                if len(next_button) > 0 and next_button[0].is_enabled():\n",
    "                    next_button[0].click()\n",
    "                    sleep(3)  # Wait for the next page to load\n",
    "                else:\n",
    "                    print(f\"No more pages or 'Next' button disabled in subsection: {subsection_url}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error during pagination: {e}\")\n",
    "                break\n",
    "\n",
    "    print(f\"Finished collecting articles for section: {section_url}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# 6. Save all collected links to a CSV file\n",
    "df = pd.DataFrame(article_links, columns=[\"Article Links\"])\n",
    "df.to_csv(\"bbc_article_links.csv\", index=False)\n",
    "\n",
    "print(f\"\\nTotal unique article links extracted: {len(article_links)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# File paths\n",
    "csv_file_path = 'Abhyudaya/Scripts and Data/bbc_article_links.csv'\n",
    "visited_links_file = 'Abhyudaya/Scripts and Data/BBC_visited_links.txt'\n",
    "txt_files_directory = 'Abhyudaya/Text Files/bbc'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(txt_files_directory):\n",
    "    os.makedirs(txt_files_directory)\n",
    "\n",
    "# Read the links from the CSV file\n",
    "def get_links(csv_file_path):\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as file:\n",
    "        return [row[0] for row in csv.reader(file) if row]\n",
    "\n",
    "# Read the visited links from visited_links.txt\n",
    "def get_visited_links(visited_links_file):\n",
    "    if os.path.exists(visited_links_file):\n",
    "        with open(visited_links_file, 'r', encoding='utf-8') as file:\n",
    "            return set(line.strip() for line in file)\n",
    "    return set()\n",
    "\n",
    "# Write the visited link to visited_links.txt\n",
    "def add_to_visited(link):\n",
    "    with open(visited_links_file, 'a', encoding='utf-8') as file:\n",
    "        file.write(link + '\\n')\n",
    "\n",
    "# Fetch content from the link and extract the relevant text using BeautifulSoup\n",
    "def fetch_and_extract_text(link):\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all divs with data-component=\"text-block\"\n",
    "        text_blocks = soup.find_all('div', {'data-component': 'text-block'})\n",
    "        if not text_blocks:\n",
    "            print(f\"No valid text-block found in {link}, skipping.\")\n",
    "            return None\n",
    "\n",
    "        # Extract text from all <p> tags inside the divs\n",
    "        paragraphs = []\n",
    "        for block in text_blocks:\n",
    "            paragraphs.extend([p.get_text() for p in block.find_all('p')])\n",
    "\n",
    "        return '\\n'.join(paragraphs)  # Combine all paragraphs into a single string\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Save content to a txt file in the specified directory\n",
    "def save_to_file(content, file_number):\n",
    "    file_path = os.path.join(txt_files_directory, f'{file_number}.txt')\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# The main logic to process the links\n",
    "links = get_links(csv_file_path)\n",
    "visited_links = get_visited_links(visited_links_file)\n",
    "\n",
    "for index, link in enumerate(links):\n",
    "    if link in visited_links:\n",
    "        print(f\"Skipping {link}, already visited.\")\n",
    "        continue\n",
    "\n",
    "    content = fetch_and_extract_text(link)\n",
    "    if content:\n",
    "        save_to_file(content, index + 1)  # Save as 1.txt, 2.txt, etc.\n",
    "        add_to_visited(link)  # Mark link as visited\n",
    "        print(f\"Saved content from {link} to {txt_files_directory}/{index + 1}.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
