{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Author**          | **Roll No**   | **Version** |\n",
    "|---------------------|----------------|--------------|\n",
    "| Abhyudaya Nair      | 24210005      | 1.0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: http://edition.cnn.com/2021/01/28/opinions/us-must-be-prepared-to-face-domestic-terror-panetta/index.html\n",
      "Saved article link: https://edition.cnn.com/business/markets-now\n",
      "Saved article link: https://edition.cnn.com/us\n",
      "Saved article link: https://edition.cnn.com/business/tech/mission-ahead\n",
      "Saved article link: https://edition.cnn.com/world/united-kingdom\n",
      "Saved article link: https://edition.cnn.com/ad-choices\n",
      "Saved article link: https://edition.cnn.com/travel/videos\n",
      "Saved article link: https://edition.cnn.com/travel\n",
      "Saved article link: https://edition.cnn.com/business/work-transformed\n",
      "Saved article link: https://edition.cnn.com/travel/destinations\n",
      "Saved article link: https://edition.cnn.com/health/life-but-better/sleep\n",
      "Saved article link: https://edition.cnn.com/world/photos\n",
      "Saved article link: https://edition.cnn.com/world/africa/inside-africa\n",
      "Saved article link: https://edition.cnn.com/style/design\n",
      "Saved article link: https://edition.cnn.com/travel/stay\n",
      "Saved article link: https://edition.cnn.com/newsletters\n",
      "Saved article link: https://edition.cnn.com/sport/football\n",
      "Saved article link: https://edition.cnn.com/entertainment/celebrities\n",
      "Saved article link: https://edition.cnn.com/sport/motorsport\n",
      "Saved article link: https://edition.cnn.com/health/life-but-better/fitness\n",
      "Saved article link: https://edition.cnn.com/profiles\n",
      "Saved article link: https://edition.cnn.com/markets/premarkets\n",
      "Saved article link: https://edition.cnn.com/terms\n",
      "Saved article link: https://edition.cnn.com/style/architecture\n",
      "Saved article link: https://edition.cnn.com/science\n",
      "Saved article link: https://edition.cnn.com/style/arts\n",
      "Saved article link: https://edition.cnn.com/weather\n",
      "Saved article link: https://edition.cnn.com/travel/food-and-drink\n",
      "Saved article link: https://edition.cnn.com/audio/podcasts/one-thing\n",
      "Saved article link: https://edition.cnn.com/interactive/asequals\n",
      "Saved article link: https://edition.cnn.com/sport/climbing\n",
      "Saved article link: https://edition.cnn.com/style/beauty\n",
      "Saved article link: https://edition.cnn.com\n",
      "Saved article link: https://edition.cnn.com/health/life-but-better/food\n",
      "Saved article link: https://edition.cnn.com/entertainment\n",
      "Saved article link: https://edition.cnn.com/specials/tv/all-shows\n",
      "Saved article link: https://edition.cnn.com/style/luxury\n",
      "Saved article link: https://edition.cnn.com/world/europe/ukraine\n",
      "Saved article link: https://edition.cnn.com/tv/schedule/cnn\n",
      "Saved article link: https://edition.cnn.com/sport/golf\n",
      "Saved article link: https://edition.cnn.com/business/tech/foreseeable-future\n",
      "Saved article link: https://edition.cnn.com/world/americas\n",
      "Saved article link: https://edition.cnn.com/markets/after-hours\n",
      "Saved article link: https://edition.cnn.com/privacy\n",
      "Saved article link: https://edition.cnn.com/world/impact-your-world\n",
      "Saved article link: https://edition.cnn.com/world/africa\n",
      "Saved article link: https://edition.cnn.com/science/unearthed\n",
      "Saved article link: https://edition.cnn.com/videos\n",
      "Saved article link: https://edition.cnn.com/world/middleeast/israel\n",
      "Saved article link: https://edition.cnn.com/science/space\n",
      "Saved article link: https://edition.cnn.com/audio/podcasts/the-assignment\n",
      "Saved article link: https://edition.cnn.com/science/life\n",
      "Saved article link: https://edition.cnn.com/world/heroes\n",
      "Saved article link: https://edition.cnn.com/interactive/life-but-better/\n",
      "Saved article link: https://edition.cnn.com/world/europe\n",
      "Saved article link: https://edition.cnn.com/climate\n",
      "Saved article link: https://edition.cnn.com/audio/podcasts/tug-of-war\n",
      "Saved article link: https://edition.cnn.com/entertainment/movies\n",
      "Saved article link: https://edition.cnn.com/audio/podcasts/chasing-life\n",
      "Saved article link: https://edition.cnn.com/video\n",
      "Saved article link: https://edition.cnn.com/interactive/call-to-earth\n",
      "Saved article link: https://edition.cnn.com/style/fashion\n",
      "Saved article link: https://edition.cnn.com/business/investing\n",
      "Saved article link: https://edition.cnn.com/sport/us-sports\n",
      "Saved article link: https://edition.cnn.com/audio/podcasts/axe-files\n",
      "Saved article link: https://edition.cnn.com/specials\n",
      "Saved article link: https://edition.cnn.com/world/china\n",
      "Saved article link: https://edition.cnn.com/world/middle-east\n",
      "Saved article link: https://edition.cnn.com/world/australia\n",
      "Saved article link: https://edition.cnn.com/audio/podcasts/all-there-is-with-anderson-cooper\n",
      "Saved article link: https://edition.cnn.com/world/asia\n",
      "Saved article link: https://edition.cnn.com/world/freedom-project\n",
      "Saved article link: https://edition.cnn.com/sport\n",
      "Saved article link: https://edition.cnn.com/transcripts\n",
      "Saved article link: https://edition.cnn.com/sport/esports\n",
      "Saved article link: https://edition.cnn.com/politics\n",
      "Saved article link: https://edition.cnn.com/style\n",
      "Saved article link: https://edition.cnn.com/us/cnn-investigates\n",
      "Saved article link: https://edition.cnn.com/business/tech/innovative-cities\n",
      "Saved article link: https://edition.cnn.com/markets/fear-and-greed\n",
      "Saved article link: https://edition.cnn.com/business/markets/nightcap\n",
      "Saved article link: https://edition.cnn.com/world/india\n",
      "Saved article link: https://edition.cnn.com/health/life-but-better/mindfulness\n",
      "Saved article link: https://edition.cnn.com/travel/news\n",
      "Saved article link: https://edition.cnn.com/audio/podcasts/political-briefing\n",
      "Saved article link: https://edition.cnn.com/business/tech/innovate\n",
      "Saved article link: https://edition.cnn.com/climate/solutions\n",
      "Saved article link: https://edition.cnn.com/profiles/cnn-leadership\n",
      "Saved article link: https://edition.cnn.com/cnn10\n",
      "Saved article link: https://edition.cnn.com/world\n",
      "Saved article link: https://edition.cnn.com/videos/fast/cnni-fast\n",
      "Saved article link: https://edition.cnn.com/markets\n",
      "Saved article link: https://edition.cnn.com/business\n",
      "Saved article link: https://edition.cnn.com/sport/paris-olympics-2024\n",
      "Saved article link: https://edition.cnn.com/business/videos\n",
      "Saved article link: https://edition.cnn.com/about\n",
      "Saved article link: https://edition.cnn.com/sport/tennis\n",
      "Saved article link: https://edition.cnn.com/live-tv\n",
      "Saved article link: https://edition.cnn.com/style/videos\n",
      "Saved article link: https://edition.cnn.com/business/media\n",
      "Saved article link: https://edition.cnn.com/business/financial-calculators\n",
      "Saved article link: https://edition.cnn.com/accessibility\n",
      "Saved article link: https://edition.cnn.com?hpt=header_edition-picker\n",
      "Saved article link: https://edition.cnn.com/audio\n",
      "Saved article link: https://edition.cnn.com/sports\n",
      "Saved article link: https://edition.cnn.com/entertainment/tv-shows\n",
      "Saved article link: https://edition.cnn.com/specials/weather/weather-video\n",
      "Saved article link: https://edition.cnn.com/audio/podcasts/5-things\n",
      "Saved article link: https://edition.cnn.com/health\n",
      "Saved article link: https://edition.cnn.com/health/life-but-better/relationships\n",
      "Saved article link: https://edition.cnn.com/business/tech\n",
      "Processing: http://edition.cnn.com/2020/09/17/opinions/rosh-hashanah-age-of-coronavrius-davidson/index.html\n",
      "Processing: http://edition.cnn.com/2020/09/24/europe/lena-headey-moria-opinion-intl/index.html\n",
      "Processing: http://edition.cnn.com/2021/02/17/opinions/texas-weather-snow-sobel/index.html\n",
      "Processing: http://edition.cnn.com/2020/10/15/opinions/supreme-court-packing-slippery-slope-olson/index.html\n",
      "Processing: http://edition.cnn.com/2015/02/13/opinion/brazile-gay-marriage-alabama/index.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_link\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m visited_links\u001b[38;5;241m.\u001b[39madd(current_link)\n\u001b[0;32m---> 96\u001b[0m \u001b[43mprocess_link\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Save progress after processing each link\u001b[39;00m\n\u001b[1;32m     99\u001b[0m save_progress()\n",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m, in \u001b[0;36mprocess_link\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_link\u001b[39m(url):\n\u001b[0;32m---> 50\u001b[0m     raw_html \u001b[38;5;241m=\u001b[39m \u001b[43mget_html_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_html \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mget_html_content\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     28\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     29\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m---> 30\u001b[0m     response\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapparent_encoding\u001b[49m  \u001b[38;5;66;03m# Set encoding based on response\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent  \u001b[38;5;66;03m# Return raw content for parsing\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/HDFS1/language_nlp/english_nlp_team17/Assignment1/.venv/lib/python3.8/site-packages/requests/models.py:793\u001b[0m, in \u001b[0;36mResponse.apparent_encoding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The apparent encoding, provided by the charset_normalizer or chardet libraries.\"\"\"\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chardet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchardet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;66;03m# If no character detection library is available, we'll fall back\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# to a standard Python utf-8 str.\u001b[39;00m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/HDFS1/language_nlp/english_nlp_team17/Assignment1/.venv/lib/python3.8/site-packages/charset_normalizer/legacy.py:36\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(byte_str, should_rename_legacy, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(byte_str, \u001b[38;5;28mbytearray\u001b[39m):\n\u001b[1;32m     34\u001b[0m     byte_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(byte_str)\n\u001b[0;32m---> 36\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbest()\n\u001b[1;32m     38\u001b[0m encoding \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     39\u001b[0m language \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m r\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/HDFS1/language_nlp/english_nlp_team17/Assignment1/.venv/lib/python3.8/site-packages/charset_normalizer/api.py:307\u001b[0m, in \u001b[0;36mfrom_bytes\u001b[0;34m(sequences, steps, chunk_size, threshold, cp_isolation, cp_exclusion, preemptive_behaviour, explain, language_threshold, enable_fallback)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m cut_sequence_chunks(\n\u001b[1;32m    294\u001b[0m     sequences,\n\u001b[1;32m    295\u001b[0m     encoding_iana,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     decoded_payload,\n\u001b[1;32m    303\u001b[0m ):\n\u001b[1;32m    304\u001b[0m     md_chunks\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m    306\u001b[0m     md_ratios\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 307\u001b[0m         \u001b[43mmess_ratio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexplain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcp_isolation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m md_ratios[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:\n\u001b[1;32m    315\u001b[0m         early_stop_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the base URL\n",
    "base_url = \"https://edition.cnn.com/\"\n",
    "\n",
    "# Define the file to store article links\n",
    "csv_file = 'Abhyudaya/Scripts and Data/cnn_article_links.csv'\n",
    "progress_file = 'Abhyudaya/Scripts and Data/cnn_links_to_traverse.txt'  # File to store progress\n",
    "\n",
    "# Store visited links to avoid reprocessing and duplicates\n",
    "visited_links = set()\n",
    "links_to_traverse = []  # Initialize as empty list\n",
    "article_links = set()  # To store unique article links\n",
    "\n",
    "# Write the article link to CSV\n",
    "def save_article_link(link):\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([link])\n",
    "\n",
    "# Function to fetch and parse the HTML content\n",
    "def get_html_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding  # Set encoding based on response\n",
    "        return response.content  # Return raw content for parsing\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to retrieve {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extracts all unique links from a given page\n",
    "def extract_links(soup, base_url):\n",
    "    links = set()\n",
    "    for anchor in soup.find_all('a', href=True):\n",
    "        href = anchor['href']\n",
    "        full_url = urljoin(base_url, href)  # Construct full URL\n",
    "\n",
    "        # Ensure the link belongs to edition.cnn.com and is a valid HTTP link\n",
    "        if \"edition.cnn.com\" in urlparse(full_url).netloc and full_url.startswith(\"http\"):\n",
    "            links.add(full_url)\n",
    "    return links\n",
    "\n",
    "# Process the current URL and extract new links\n",
    "def process_link(url):\n",
    "    raw_html = get_html_content(url)\n",
    "    if raw_html is None:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')  # Attempt to parse HTML\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error for {url}: {e}\")\n",
    "        return  # Skip this link and continue\n",
    "\n",
    "    # Extract all unique links from the current page\n",
    "    links = extract_links(soup, url)\n",
    "\n",
    "    # Filter and save article links that start with \"https://edition.cnn.com\"\n",
    "    for link in links:\n",
    "        if link.startswith(\"https://edition.cnn.com\") and link not in article_links:\n",
    "            article_links.add(link)\n",
    "            save_article_link(link)\n",
    "            print(f\"Saved article link: {link}\")\n",
    "\n",
    "    # Add unvisited links to the links_to_traverse list\n",
    "    for link in links:\n",
    "        if link not in visited_links:\n",
    "            links_to_traverse.append(link)\n",
    "\n",
    "# Load previously saved links from the progress file\n",
    "def load_progress():\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as file:\n",
    "            return [line.strip() for line in file.readlines()]\n",
    "    return []\n",
    "\n",
    "# Save the current state of links_to_traverse to the progress file\n",
    "def save_progress():\n",
    "    with open(progress_file, 'w') as file:\n",
    "        for link in links_to_traverse:\n",
    "            file.write(link + '\\n')\n",
    "\n",
    "# Main crawling loop\n",
    "links_to_traverse = load_progress() or [base_url]  # Load progress or start fresh\n",
    "\n",
    "while links_to_traverse:\n",
    "    current_link = links_to_traverse.pop(0)\n",
    "    if current_link not in visited_links:\n",
    "        print(f\"Processing: {current_link}\")\n",
    "        visited_links.add(current_link)\n",
    "        process_link(current_link)\n",
    "        \n",
    "        # Save progress after processing each link\n",
    "        save_progress()\n",
    "\n",
    "# Final save of progress when done\n",
    "save_progress()\n",
    "print(\"Crawling complete. Progress saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# File paths\n",
    "csv_file_path = 'Assignment1/Abhyudaya/Scripts and Data/cnn_article_links.csv'\n",
    "visited_links_file = 'Assignment1/Abhyudaya/Scripts and Data/CNN_visited_links.txt'\n",
    "txt_files_directory = 'Assignment1/Abhyudaya/Data/cnn'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(txt_files_directory):\n",
    "    os.makedirs(txt_files_directory)\n",
    "\n",
    "# Read the links from the CSV file\n",
    "def get_links(csv_file_path):\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as file:\n",
    "        return [row[0] for row in csv.reader(file) if row]\n",
    "\n",
    "# Read the visited links from visited_links.txt\n",
    "def get_visited_links(visited_links_file):\n",
    "    if os.path.exists(visited_links_file):\n",
    "        with open(visited_links_file, 'r', encoding='utf-8') as file:\n",
    "            return set(line.strip() for line in file)\n",
    "    return set()\n",
    "\n",
    "# Write the visited link to visited_links.txt\n",
    "def add_to_visited(link):\n",
    "    with open(visited_links_file, 'a', encoding='utf-8') as file:\n",
    "        file.write(link + '\\n')\n",
    "\n",
    "# Fetch content from the link and extract text using BeautifulSoup\n",
    "def fetch_and_extract_text(link):\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the div with class=\"article__content\"\n",
    "        content_div = soup.find('div', {'class': 'article__content'})\n",
    "        if not content_div:\n",
    "            print(f\"No 'article__content' div found in {link}, skipping.\")\n",
    "            return None\n",
    "\n",
    "        # Extract text from all <p> tags inside the div\n",
    "        paragraphs = [p.get_text() for p in content_div.find_all('p')]\n",
    "        if not paragraphs:\n",
    "            print(f\"No <p> tags found in 'article__content' div of {link}, skipping.\")\n",
    "            return None\n",
    "\n",
    "        return '\\n'.join(paragraphs)  # Combine all paragraphs into a single string\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {link}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Save content to a txt file in the specified directory\n",
    "def save_to_file(content, file_number):\n",
    "    file_path = os.path.join(txt_files_directory, f'cnn_{file_number}.txt')\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# The main logic to process the links\n",
    "links = get_links(csv_file_path)\n",
    "visited_links = get_visited_links(visited_links_file)\n",
    "\n",
    "for index, link in enumerate(links):\n",
    "    if link in visited_links:\n",
    "        print(f\"Skipping {link}, already visited.\")\n",
    "        continue\n",
    "\n",
    "    content = fetch_and_extract_text(link)\n",
    "    if content:\n",
    "        save_to_file(content, index + 1)  # Save as cnn_1.txt, cnn_2.txt, etc.\n",
    "        add_to_visited(link)  # Mark link as visited\n",
    "        print(f\"Saved content from {link} to {txt_files_directory}/cnn_{index + 1}.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
